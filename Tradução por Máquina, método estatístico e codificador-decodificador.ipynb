{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed281b69-6941-4746-8fcb-edb34d3f03b5",
      "metadata": {
        "id": "ed281b69-6941-4746-8fcb-edb34d3f03b5"
      },
      "source": [
        "# Tradução por Máquina: método estatístico e codificador-decodificador"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c599ca7c-fbff-4dd6-a4a3-553a0feb60ac",
      "metadata": {
        "id": "c599ca7c-fbff-4dd6-a4a3-553a0feb60ac"
      },
      "source": [
        "### Trabalho 3 da disciplina Noções de Inteligência Artificial - 2/2024\n",
        "### Alunos: Felipe Lopes Gibin Duarte (231025207) e Matheus das Neves Fernandes (231013672)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e92ee94-5f59-466b-aa2b-818f1e810f9e",
      "metadata": {
        "id": "9e92ee94-5f59-466b-aa2b-818f1e810f9e"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a562ae-df7e-47c3-851d-a72321bc1fe1",
      "metadata": {
        "id": "e3a562ae-df7e-47c3-851d-a72321bc1fe1"
      },
      "source": [
        "Neste trabalho vamos usar duas técnicas de tradução automática, a primeira puramente\n",
        "estatística e a segunda usando ML na arquitetura codificador-decodificador. Vamos analisar a eficácia de ambas, destacando aspectos teóricos importantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7409cbdf-0f8b-4718-8e7f-cb26a9df494b",
      "metadata": {
        "id": "7409cbdf-0f8b-4718-8e7f-cb26a9df494b"
      },
      "source": [
        "## 1. Base de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53ee5a2-9de8-4599-9a0d-248e6d192073",
      "metadata": {
        "id": "b53ee5a2-9de8-4599-9a0d-248e6d192073"
      },
      "source": [
        "### 1.1 Download da base de dados\n",
        "\n",
        "Usaremos a base Português - Inglês do parlamento europeu, disponível em https://www.statmt.org/europarl/. Encontre o link “parallel corpus Portuguese-English” e extraia o condeúdo do zip. Pela falta de recursos computacionais, o tamanho dos arquivos foi reduzido para 11 mil sentenças cada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7887fa99-d614-4840-b9c7-5c59b543eb2b",
      "metadata": {
        "id": "7887fa99-d614-4840-b9c7-5c59b543eb2b"
      },
      "source": [
        "### 1.2 Upload dos arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0e788379-61d8-4e48-b74d-0b8ccb15e670",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "0e788379-61d8-4e48-b74d-0b8ccb15e670",
        "outputId": "45f1ffdd-b700-4e5d-a63e-560d91b33570"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-55cb3c6b-8398-4284-a315-98c309cd22bb\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-55cb3c6b-8398-4284-a315-98c309cd22bb\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving reduced_europarl_pt.txt to reduced_europarl_pt.txt\n",
            "Saving reduced_europarl_en.txt to reduced_europarl_en.txt\n"
          ]
        }
      ],
      "source": [
        "#Upload dos arquivos extraídos para o google colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Processamento dos arquivos"
      ],
      "metadata": {
        "id": "JQhuNIwr7YEZ"
      },
      "id": "JQhuNIwr7YEZ"
    },
    {
      "cell_type": "code",
      "source": [
        "#Limpeza das sentenças\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'-', ' ',text) #trata hifens\n",
        "    text = re.sub(r'[^\\w\\s]', '', text) # remove pontuação\n",
        "    return text.strip()"
      ],
      "metadata": {
        "id": "eY62gshE2_pA"
      },
      "id": "eY62gshE2_pA",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Le os arquivos e remove line breaks\n",
        "with open('/content/reduced_europarl_pt.txt', 'r', encoding='utf-8') as pt_file:\n",
        "    portuguese_sentences = [clean_text(line.strip()) for line in pt_file.readlines()]\n",
        "\n",
        "with open('/content/reduced_europarl_en.txt', 'r', encoding='utf-8') as en_file:\n",
        "    english_sentences = [clean_text(line.strip()) for line in en_file.readlines()]\n",
        "\n",
        "\n",
        "portuguese_sentences= portuguese_sentences[:10000]\n",
        "english_sentences= english_sentences[:10000]\n",
        "assert len(portuguese_sentences) == len(english_sentences)\n",
        "\n",
        "# Exibir algumas linhas para verificar\n",
        "print(\"Frases em português:\", portuguese_sentences[:5])\n",
        "print(\"Frases em inglês:\", english_sentences[:5])"
      ],
      "metadata": {
        "id": "nh4VrD6y2MlX",
        "outputId": "d6d3e108-0aee-4de3-f476-39d9b29b0e11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "nh4VrD6y2MlX",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frases em português: ['reinício da sessão', 'declaro reaberta a sessão do parlamento europeu que tinha sido interrompida na sexta feira 17 de dezembro último e renovo todos os meus votos esperando que tenham tido boas férias', 'como puderam constatar o grande bug do ano 2000 não aconteceu em contrapartida os cidadãos de alguns dos nossos países foram vítimas de catástrofes naturais verdadeiramente terríveis', 'os senhores manifestaram o desejo de se proceder a um debate sobre o assunto nos próximos dias durante este período de sessões', 'entretanto gostaria   como também me foi pedido por um certo número de colegas   que observássemos um minuto de silêncio por todas as vítimas nomeadamente das tempestades nos diferentes países da união europeia que foram afectados']\n",
            "Frases em inglês: ['resumption of the session', 'i declare resumed the session of the european parliament adjourned on friday 17 december 1999 and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period', 'although as you will have seen the dreaded millennium bug failed to materialise still the people in a number of countries suffered a series of natural disasters that truly were dreadful', 'you have requested a debate on this subject in the course of the next few days during this part session', 'in the meantime i should like to observe a minute s silence as a number of members have requested on behalf of all the victims concerned particularly those of the terrible storms in the various countries of the european union']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f7e8c41-2c2c-4564-b4ea-87e534a30b4e",
      "metadata": {
        "id": "3f7e8c41-2c2c-4564-b4ea-87e534a30b4e"
      },
      "source": [
        "## 2. IBM Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f58d0ff-9cbc-40d8-87a4-14304f2cc5c5",
      "metadata": {
        "id": "5f58d0ff-9cbc-40d8-87a4-14304f2cc5c5"
      },
      "source": [
        "O IBM Model 1 é um modelo complexo de tradução estatística, amplamente utilizado antes da popularização das traduções baseadas em redes neurais. Ele se baseia em probabilidades de palavras individuais e sua correspondência entre os idiomas, ou seja, o modelo estima a melhor tradução palavra por palavra."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Treinando o modelo IBM Model 1"
      ],
      "metadata": {
        "id": "QjcsqBCGASqp"
      },
      "id": "QjcsqBCGASqp"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "d459c5dd-aa36-4e12-ab7c-f34e7b6fc6d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d459c5dd-aa36-4e12-ab7c-f34e7b6fc6d3",
        "outputId": "3273e618-e4fa-4aed-80b9-a263e915a202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Português (fonte): [['reinício', 'da', 'sessão'], ['declaro', 'reaberta', 'a', 'sessão', 'do', 'parlamento', 'europeu', 'que', 'tinha', 'sido', 'interrompida', 'na', 'sexta', 'feira', '17', 'de', 'dezembro', 'último', 'e', 'renovo', 'todos', 'os', 'meus', 'votos', 'esperando', 'que', 'tenham', 'tido', 'boas', 'férias'], ['como', 'puderam', 'constatar', 'o', 'grande', 'bug', 'do', 'ano', '2000', 'não', 'aconteceu', 'em', 'contrapartida', 'os', 'cidadãos', 'de', 'alguns', 'dos', 'nossos', 'países', 'foram', 'vítimas', 'de', 'catástrofes', 'naturais', 'verdadeiramente', 'terríveis'], ['os', 'senhores', 'manifestaram', 'o', 'desejo', 'de', 'se', 'proceder', 'a', 'um', 'debate', 'sobre', 'o', 'assunto', 'nos', 'próximos', 'dias', 'durante', 'este', 'período', 'de', 'sessões'], ['entretanto', 'gostaria', 'como', 'também', 'me', 'foi', 'pedido', 'por', 'um', 'certo', 'número', 'de', 'colegas', 'que', 'observássemos', 'um', 'minuto', 'de', 'silêncio', 'por', 'todas', 'as', 'vítimas', 'nomeadamente', 'das', 'tempestades', 'nos', 'diferentes', 'países', 'da', 'união', 'europeia', 'que', 'foram', 'afectados']]\n",
            "Inglês (alvo): [['resumption', 'of', 'the', 'session'], ['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period'], ['although', 'as', 'you', 'will', 'have', 'seen', 'the', 'dreaded', 'millennium', 'bug', 'failed', 'to', 'materialise', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful'], ['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', 'during', 'this', 'part', 'session'], ['in', 'the', 'meantime', 'i', 'should', 'like', 'to', 'observe', 'a', 'minute', 's', 'silence', 'as', 'a', 'number', 'of', 'members', 'have', 'requested', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union']]\n"
          ]
        }
      ],
      "source": [
        "#Instalando o nltk\n",
        "!pip install nltk\n",
        "\n",
        "# Tokenização\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "portuguese_tokens = [word_tokenize(sentence, language='portuguese') for sentence in portuguese_sentences]\n",
        "english_tokens = [word_tokenize(sentence, language='english') for sentence in english_sentences]\n",
        "\n",
        "assert len(portuguese_tokens) == len(english_tokens)\n",
        "\n",
        "# Verificar tokens das primeiras sentenças\n",
        "print(\"Português (fonte):\", portuguese_tokens[:5])\n",
        "print(\"Inglês (alvo):\", english_tokens[:5])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Criar o corpus alinhado em pares de listas do tipo (target, source) → (Inglês, Português)\n",
        "from nltk.translate import AlignedSent\n",
        "parallel_corpus = [AlignedSent(en,pt) for pt, en in zip(portuguese_tokens, english_tokens)]\n",
        "\n",
        "print(parallel_corpus[:5])"
      ],
      "metadata": {
        "id": "fHU9tEFJ3UyS",
        "outputId": "c28f83c0-a2ba-4e7e-f06b-b94e2237de92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fHU9tEFJ3UyS",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[AlignedSent(['resumption', 'of', 'the', 'session'], ['reinício', 'da', 'sessão'], Alignment([])), AlignedSent(['i', 'declare', 'resumed', 'the', 'session', 'of', 'the', 'european', 'parliament', 'adjourned', 'on', 'friday', '17', 'december', '1999', 'and', 'i', 'would', 'like', 'once', 'again', 'to', 'wish', 'you', 'a', 'happy', 'new', 'year', 'in', 'the', 'hope', 'that', 'you', 'enjoyed', 'a', 'pleasant', 'festive', 'period'], ['declaro', 'reaberta', 'a', 'sessão', 'do', 'parlamento', 'europeu', 'que', 'tinha', 'sido', 'interrompida', 'na', 'sexta', 'feira', '17', 'de', 'dezembro', 'último', 'e', 'renovo', 'todos', 'os', 'meus', 'votos', 'esperando', 'que', 'tenham', 'tido', 'boas', 'férias'], Alignment([])), AlignedSent(['although', 'as', 'you', 'will', 'have', 'seen', 'the', 'dreaded', 'millennium', 'bug', 'failed', 'to', 'materialise', 'still', 'the', 'people', 'in', 'a', 'number', 'of', 'countries', 'suffered', 'a', 'series', 'of', 'natural', 'disasters', 'that', 'truly', 'were', 'dreadful'], ['como', 'puderam', 'constatar', 'o', 'grande', 'bug', 'do', 'ano', '2000', 'não', 'aconteceu', 'em', 'contrapartida', 'os', 'cidadãos', 'de', 'alguns', 'dos', 'nossos', 'países', 'foram', 'vítimas', 'de', 'catástrofes', 'naturais', 'verdadeiramente', 'terríveis'], Alignment([])), AlignedSent(['you', 'have', 'requested', 'a', 'debate', 'on', 'this', 'subject', 'in', 'the', 'course', 'of', 'the', 'next', 'few', 'days', 'during', 'this', 'part', 'session'], ['os', 'senhores', 'manifestaram', 'o', 'desejo', 'de', 'se', 'proceder', 'a', 'um', 'debate', 'sobre', 'o', 'assunto', 'nos', 'próximos', 'dias', 'durante', 'este', 'período', 'de', 'sessões'], Alignment([])), AlignedSent(['in', 'the', 'meantime', 'i', 'should', 'like', 'to', 'observe', 'a', 'minute', 's', 'silence', 'as', 'a', 'number', 'of', 'members', 'have', 'requested', 'on', 'behalf', 'of', 'all', 'the', 'victims', 'concerned', 'particularly', 'those', 'of', 'the', 'terrible', 'storms', 'in', 'the', 'various', 'countries', 'of', 'the', 'european', 'union'], ['entretanto', 'gostaria', 'como', 'também', 'me', 'foi', 'pedido', 'por', 'um', 'certo', 'número', 'de', 'colegas', 'que', 'observássemos', 'um', 'minuto', 'de', 'silêncio', 'por', 'todas', 'as', 'vítimas', 'nomeadamente', 'das', 'tempestades', 'nos', 'diferentes', 'países', 'da', 'união', 'europeia', 'que', 'foram', 'afectados'], Alignment([]))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Treinamento do Modelo (direção correta)\n",
        "from nltk.translate import IBMModel1\n",
        "model = IBMModel1(parallel_corpus, 20)  # 20 iterações\n",
        "\n",
        "print(\"Modelo treinado com sucesso!\")"
      ],
      "metadata": {
        "id": "hmfDXfmC3hu0",
        "outputId": "6367e2df-a800-41a1-aaa0-cea83134e3a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hmfDXfmC3hu0",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo treinado com sucesso!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Avaliando a qualidade da tradução"
      ],
      "metadata": {
        "id": "UlNbYdnfDOjO"
      },
      "id": "UlNbYdnfDOjO"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "0e2f9554-530b-4f31-9805-8b7b5fb0c046",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "0e2f9554-530b-4f31-9805-8b7b5fb0c046",
        "outputId": "1f877306-b4fe-4dd5-8fec-9e3e0f7629c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dicionário de tradução (exemplo): [(None, ('the', 0.36681815178192073)), ('poderá', ('can', 0.3143122775494104)), ('a', ('the', 0.2625850446893964)), ('comissão', ('commission', 0.9955597345022094)), ('assegurar', ('ensure', 0.7091438485543141))]\n",
            "10716\n",
            "11592\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "The number of hypotheses and their reference(s) should be the same ",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-42ab13a9b6de>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# 5. Calcular o BLEU score médio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mbleu_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhypotheses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"BLEU Score médio: {bleu_score:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[0mhyp_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     assert len(list_of_references) == len(hypotheses), (\n\u001b[0m\u001b[1;32m    221\u001b[0m         \u001b[0;34m\"The number of hypotheses and their reference(s) should be the \"\u001b[0m \u001b[0;34m\"same \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     )\n",
            "\u001b[0;31mAssertionError\u001b[0m: The number of hypotheses and their reference(s) should be the same "
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
        "import numpy as np\n",
        "\n",
        "# Função de suavização para o BLEU score\n",
        "smoothing = SmoothingFunction().method1\n",
        "\n",
        "# 1. Criar o dicionário de tradução\n",
        "translation_dict = {}\n",
        "for en_word in model.translation_table:\n",
        "    for pt_word in model.translation_table[en_word]:\n",
        "        prob = model.translation_table[en_word][pt_word]\n",
        "        if pt_word not in translation_dict or prob > translation_dict[pt_word][1]:\n",
        "            translation_dict[pt_word] = (en_word, prob)\n",
        "\n",
        "# Exibir uma parte do dicionário de tradução\n",
        "print(\"Dicionário de tradução (exemplo):\", list(translation_dict.items())[:5])\n",
        "\n",
        "# 2. Função de Tradução\n",
        "def translate(source_tokens):\n",
        "    return [translation_dict.get(tok, ('UNK', 0))[0] for tok in source_tokens]\n",
        "\n",
        "# Vamos usar as sentenças em inglês como referências e as sentenças traduzidas como hipóteses\n",
        "references = [[eng] for eng in english_tokens]\n",
        "hypotheses = [translate(pt) for pt in portuguese_tokens]\n",
        "\n",
        "#debugging\n",
        "print(len(references))\n",
        "print(len(hypotheses))\n",
        "\n",
        "\n",
        "# 5. Calcular o BLEU score médio\n",
        "bleu_score = corpus_bleu(references, hypotheses, weights=(0.5, 0.5), smoothing_function=smoothing)\n",
        "print(f\"BLEU Score médio: {bleu_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplo: Ver traduções para a palavra \"sim\"\n",
        "word = \"sim\"\n",
        "if word in model.translation_table:\n",
        "    translations = model.translation_table[word]\n",
        "    sorted_translations = sorted(translations.items(), key=lambda x: -x[1])\n",
        "    print(f\"Traduções para '{word}': {sorted_translations[:5]}\")  # Top 5\n",
        "else:\n",
        "    print(f\"'{word}' não encontrada no modelo.\")"
      ],
      "metadata": {
        "id": "poRy2J0a7c4w",
        "outputId": "4225dc77-a9d2-420e-adca-0a4a3eea74f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "poRy2J0a7c4w",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traduções para 'sim': [('yes', 0.6672324401779459), ('deflation', 0.19811735634075164), ('definite', 0.16528434773779155), ('philosophical', 0.16401117047168076), ('justification', 0.1572788962025973)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tradução de uma frase\n",
        "sentence1=portuguese_tokens[0]\n",
        "sentence2=portuguese_tokens[8]\n",
        "\n",
        "print(sentence1)\n",
        "print(translate_sentence(model,sentence1))\n",
        "print(\"-\" * 50)\n",
        "\n",
        "print(sentence2)\n",
        "print(translate_sentence(model,sentence2))\n",
        "print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "PJ33U0NMM84F",
        "outputId": "ce5c25c5-7530-419c-c358-07dde870e452",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PJ33U0NMM84F",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['reinício', 'da', 'sessão']\n",
            "['resumption', 's', 'session']\n",
            "--------------------------------------------------\n",
            "['certamente', 'que', 'já', 'tomou', 'conhecimento', 'pelas', 'notícias', 'transmitidas', 'na', 'imprensa', 'e', 'na', 'televisão', 'dos', 'diversos', 'atentados', 'à', 'bomba', 'e', 'assassínios', 'perpetrados', 'no', 'sri', 'lanka']\n",
            "['certainly', 'which', 'already', 'seizing', 'knowledge', 'determined', 'news', 'relayed', 'afsj', 'press', 'and', 'afsj', 'television', 'vessels', 'various', 'explosions', 'starts', 'bomb', 'and', 'murders', 'explosions', 'within', 'lanka', 'lanka']\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Considerações sobre o BLEU score"
      ],
      "metadata": {
        "id": "FYubUAnfQK9y"
      },
      "id": "FYubUAnfQK9y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "O BLEU score (Bilingual Evaluation Understudy) é uma métrica usada para avaliar a qualidade de traduções automáticas, comparando a tradução gerada pelo modelo com uma ou mais traduções de referência. No caso do IBM model 1, o resultado encontrado foi desapontador devido à abordagem de tradução palavra a palavra, que é ineficiente. Alem disso, a limitação em tamanho da base de dados devido à restrições de capacidade computacional foi outro fator relevante para o score médio ter sido tão baixo (0.0294).  "
      ],
      "metadata": {
        "id": "WyOLAzhBQZD6"
      },
      "id": "WyOLAzhBQZD6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Codificador-Decodificador"
      ],
      "metadata": {
        "id": "uZ1NvUh2RqH0"
      },
      "id": "uZ1NvUh2RqH0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Upload e processamento dos dados"
      ],
      "metadata": {
        "id": "mANyV_sRCUoP"
      },
      "id": "mANyV_sRCUoP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir algumas linhas para verificar\n",
        "print(\"Frases em português:\", portuguese_sentences[:5])\n",
        "print(\"Frases em inglês:\", english_sentences[:5])"
      ],
      "metadata": {
        "id": "w43QCYY1DEKT",
        "outputId": "14f9f2ff-9b94-45af-fcef-4ea8c4dcef5b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w43QCYY1DEKT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frases em português: ['Reinício da sessão', 'Declaro reaberta a sessão do Parlamento Europeu, que tinha sido interrompida na sexta-feira, 17 de Dezembro último, e renovo todos os meus votos, esperando que tenham tido boas férias.', 'Como puderam constatar, o grande \"bug do ano 2000\" não aconteceu. Em contrapartida, os cidadãos de alguns dos nossos países foram vítimas de catástrofes naturais verdadeiramente terríveis.', 'Os senhores manifestaram o desejo de se proceder a um debate sobre o assunto nos próximos dias, durante este período de sessões.', 'Entretanto, gostaria - como também me foi pedido por um certo número de colegas - que observássemos um minuto de silêncio por todas as vítimas, nomeadamente das tempestades, nos diferentes países da União Europeia que foram afectados.']\n",
            "Frases em inglês: ['Resumption of the session', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.', \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Importações e configurações\n"
      ],
      "metadata": {
        "id": "FuKiFoj4D_tV"
      },
      "id": "FuKiFoj4D_tV"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Input\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Configurações\n",
        "max_vocab_size = 25000  # Tamanho máximo do vocabulário\n",
        "max_seq_length = 30     # Tamanho máximo da sequência\n",
        "embedding_dim = 256     # Dimensão do embedding\n",
        "latent_dim = 512        # Dimensão do vetor latente (de contexto) do RNN\n",
        "batch_size = 64\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "lneHetUNECjg"
      },
      "id": "lneHetUNECjg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Tokenização e padding"
      ],
      "metadata": {
        "id": "MHuDJz-kETMA"
      },
      "id": "MHuDJz-kETMA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenização converte palavras em números, onde cada número representa o índice de uma palavra no vocabulário. Palavras associadas a índices de número mais baixo são mais frequentes no texto. O token \"start\" e \"end\" indicam onde a tradução começa e onde termina. O padding garante que todas as sentenças tenham o mesmo comprimento, se for uma frase mais curta que max__seq_length, serão adicionados zeros ao final das frases."
      ],
      "metadata": {
        "id": "CROF6ckNXiqk"
      },
      "id": "CROF6ckNXiqk"
    },
    {
      "cell_type": "code",
      "source": [
        "#Adicionamos tokens de inicio e fim em ingles\n",
        "english_sentences = [\"<start> \" + sentence.strip() + \" <end>\" for sentence in english_sentences]\n",
        "print(english_sentences[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZxoWZu3YitB",
        "outputId": "66befd5b-5ad9-4bda-b25e-e614fedb7a22"
      },
      "id": "0ZxoWZu3YitB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenização\n",
        "tokenizer_pt = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer_pt.fit_on_texts(portuguese_sentences)\n",
        "\n",
        "tokenizer_en = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer_en.fit_on_texts(english_sentences)"
      ],
      "metadata": {
        "id": "329Rfw94EW4C"
      },
      "id": "329Rfw94EW4C",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequências numericas das sentenças em portugues e ingles\n",
        "input_sequences = tokenizer_pt.texts_to_sequences(portuguese_sentences)\n",
        "target_sequences = tokenizer_en.texts_to_sequences(english_sentences)"
      ],
      "metadata": {
        "id": "VAwr3oP4FDrM"
      },
      "id": "VAwr3oP4FDrM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adiciona padding ao final das sentenças, de forma a padronizar o tamanho delas\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding=\"post\")\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_seq_length, padding=\"post\")\n"
      ],
      "metadata": {
        "id": "Rd-fo7OjFTe0"
      },
      "id": "Rd-fo7OjFTe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criação dos arrays de entrada e saida\n",
        "encoder_input_data = np.array(input_sequences)\n",
        "decoder_input_data = pad_sequences(target_sequences[:, :-1], maxlen=max_seq_length, padding=\"post\")  # Sem o token <end>\n",
        "decoder_target_data = pad_sequences(target_sequences[:, 1:], maxlen=max_seq_length, padding=\"post\")  # Sem o token <start>\n",
        "\n",
        "# Corrigindo erro de tamanho dos arrays\n",
        "min_samples = min(len(encoder_input_data), len(decoder_input_data), len(decoder_target_data))\n",
        "\n",
        "encoder_input_data = encoder_input_data[:min_samples]\n",
        "decoder_input_data = decoder_input_data[:min_samples]\n",
        "decoder_target_data = decoder_target_data[:min_samples]\n",
        "\n",
        "# Verifique novamente os tamanhos\n",
        "print(f\"encoder_input_data shape: {encoder_input_data.shape}\")\n",
        "print(f\"decoder_input_data shape: {decoder_input_data.shape}\")\n",
        "print(f\"decoder_target_data shape: {decoder_target_data.shape}\")"
      ],
      "metadata": {
        "id": "FlE5--vZeeQg"
      },
      "id": "FlE5--vZeeQg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Estrutura do modelo encoder-decoder"
      ],
      "metadata": {
        "id": "DwMCX73mcd7Z"
      },
      "id": "DwMCX73mcd7Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "O encoder processa a sequência de entrada (sentenças em português) e gera um vetor de contexto que contém a representação da sequência. O vetor de contexto é então passado para o decodificador. Neste caso, encoder_outputs pode ser ignorado, só estamos interessados no vetor state_h."
      ],
      "metadata": {
        "id": "DgdHzOINc0aE"
      },
      "id": "DgdHzOINc0aE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o encoder\n",
        "\n",
        "# A entrada é uma sequência de índices (tokens) representando palavras em português\n",
        "encoder_inputs = Input(shape=(max_seq_length,))\n",
        "\n",
        "#Embedding para representar cada palavra como um vetor\n",
        "encoder_embedding = Embedding(input_dim=max_vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
        "\n",
        "#RNN que vai gerar o vetor de estado\n",
        "encoder_rnn = SimpleRNN(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h = encoder_rnn(encoder_embedding)"
      ],
      "metadata": {
        "id": "7-KpaTeVFljs"
      },
      "id": "7-KpaTeVFljs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O decoder usa o estado latente (state_h) gerado pelo encoder para produzir a tradução. Ele precisa ser alimentado com o token \"start\" no início, gerando uma palavra por vez até encontrar o token \"end\"."
      ],
      "metadata": {
        "id": "n4Wty8jNgNE2"
      },
      "id": "n4Wty8jNgNE2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o decoder\n",
        "\n",
        "#Sequencia de tokens em ingles, sem o <end>\n",
        "decoder_inputs = Input(shape=(max_seq_length,))\n",
        "\n",
        "#Embedding dos tokens em ingles\n",
        "decoder_embedding = Embedding(input_dim=max_vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
        "\n",
        "#RNN do decoder\n",
        "#decoder_outputs: Saída para cada passo temporal da RNN no decoder.\n",
        "#A RNN usa o estado final do encoder (state_h) como estado inicial.\n",
        "decoder_rnn = SimpleRNN(latent_dim, return_sequences=True)\n",
        "decoder_outputs = decoder_rnn(decoder_embedding, initial_state=state_h)\n",
        "\n",
        "#Camada densa para prever a próxima palavra\n",
        "# A softmax prevê a próxima palavra em inglês como uma probabilidade sobre o vocabulário\n",
        "decoder_dense = Dense(max_vocab_size, activation=\"softmax\",kernel_regularizer=regularizers.l2(0.01))\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "ErJmpm55ihec"
      },
      "id": "ErJmpm55ihec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, combinamos o encoder e o decoder para montar o modelo completo que mapeará as frases em ingles para as suas traduções em inglês."
      ],
      "metadata": {
        "id": "YsNwpU_1lhR5"
      },
      "id": "YsNwpU_1lhR5"
    },
    {
      "cell_type": "code",
      "source": [
        "#O modelo recebe como entrada encoder_inputs e decoder_inputs e tem como saida decoder_outputs (frases em ingles traduzidas)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "X__WpG14l8BW"
      },
      "id": "X__WpG14l8BW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Treinamento"
      ],
      "metadata": {
        "id": "H4YKgGBDm7-J"
      },
      "id": "H4YKgGBDm7-J"
    },
    {
      "cell_type": "code",
      "source": [
        "#Compilação do modelo completo\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "8yXhbiQnn_f-"
      },
      "id": "8yXhbiQnn_f-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinamento do modelo\n",
        "\n",
        "#Early stopping\n",
        "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split = 0.2,  # 20% para validação.\n",
        "    callbacks=[es]\n",
        ")"
      ],
      "metadata": {
        "id": "GUB47Y9VoAIP",
        "outputId": "ebe39912-da3a-459e-ed94-6bff3ff5ae15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        }
      },
      "id": "GUB47Y9VoAIP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 83ms/step - accuracy: 0.5256 - loss: 2.4318 - val_accuracy: 0.4339 - val_loss: 4.0142\n",
            "Epoch 2/20\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 79ms/step - accuracy: 0.5405 - loss: 2.3184 - val_accuracy: 0.4346 - val_loss: 4.0479\n",
            "Epoch 3/20\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 77ms/step - accuracy: 0.5548 - loss: 2.2103 - val_accuracy: 0.4343 - val_loss: 4.0797\n",
            "Epoch 4/20\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 73ms/step - accuracy: 0.5728 - loss: 2.0962 - val_accuracy: 0.4317 - val_loss: 4.1222\n",
            "Epoch 5/20\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 80ms/step - accuracy: 0.5890 - loss: 1.9943 - val_accuracy: 0.4285 - val_loss: 4.1543\n",
            "Epoch 6/20\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 73ms/step - accuracy: 0.6007 - loss: 1.9252 - val_accuracy: 0.4323 - val_loss: 4.1938\n",
            "Epoch 7/20\n",
            "\u001b[1m134/134\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 74ms/step - accuracy: 0.6193 - loss: 1.8216 - val_accuracy: 0.4275 - val_loss: 4.2353\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'accuracy' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-a078bc0859d6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Printar a acurácia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'accuracy' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Tradução de frases simples"
      ],
      "metadata": {
        "id": "Ym_SeARYrPHc"
      },
      "id": "Ym_SeARYrPHc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A arquitetura que criamos é para o treinamento. Para usar o modelo treinado na prática, precisamos configurar o encoder e decoder para inferência. Na inferência (tradução), o processo é diferente porque não temos a frase-alvo completa (em inglês). A tradução precisa ser gerada uma palavra por vez, de forma iterativa."
      ],
      "metadata": {
        "id": "7WOY_QyxtQqE"
      },
      "id": "7WOY_QyxtQqE"
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder para inferencia\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "metadata": {
        "id": "aPwztc8ouSCi",
        "outputId": "99a6e400-18c6-411b-a8a8-c2ef0a21de53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "id": "aPwztc8ouSCi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'encoder_states' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-04a691082049>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Encoder para inferencia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mencoder_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder_states' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder para inferencia\n",
        "#Recebe os estados latentes do encoder como entrada e gera palavras passo a passo\n",
        "\n",
        "# Entradas para os estados iniciais do decoder\n",
        "decoder_state_input = Input(shape=(latent_dim,))\n",
        "\n",
        "# Entrada da palavra atual\n",
        "decoder_single_input = Input(shape=(1,))  # Entrada com uma palavra por vez\n",
        "\n",
        "# Embedding para a palavra atual\n",
        "decoder_embedded = decoder_embedding(decoder_single_input)\n",
        "\n",
        "# RNN usando o estado anterior\n",
        "decoder_output, decoder_state = decoder_rnn(decoder_embedded, initial_state=decoder_state_input)\n",
        "\n",
        "# Camada densa para prever a próxima palavra\n",
        "decoder_prediction = decoder_dense(decoder_output)\n",
        "\n",
        "# Modelo do decoder para inferência\n",
        "decoder_model = Model(\n",
        "    [decoder_single_input, decoder_state_input],  # Entradas: palavra + estado anterior\n",
        "    [decoder_prediction, decoder_state]          # Saídas: previsão + estado atual\n",
        ")"
      ],
      "metadata": {
        "id": "xFrfez88ubyG"
      },
      "id": "xFrfez88ubyG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criaremos uma função de tradução que passa uma frase em portugûes e retorna a frase traduzida para o inglês. Após isso, traduziremos uma frase simples"
      ],
      "metadata": {
        "id": "tasAEIpGrUmB"
      },
      "id": "tasAEIpGrUmB"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(input_sequence, encoder_model, decoder_model, tokenizer_en, max_seq_length):\n",
        "    # Obtenha o estado latente do encoder\n",
        "    encoder_states = encoder_model.predict(input_sequence)\n",
        "\n",
        "    # Comece a tradução com o token <start>\n",
        "    target_sequence = np.zeros((1, 1))  # Tamanho (1 frase, 1 palavra)\n",
        "    target_sequence[0, 0] = tokenizer_en.word_index[\"<start>\"]\n",
        "\n",
        "    decoded_sentence = []\n",
        "\n",
        "    # Iterar para gerar palavras até o token <end> ou o limite de comprimento\n",
        "    for _ in range(max_seq_length):\n",
        "        # Previsão da próxima palavra\n",
        "        output_tokens, h = decoder_model.predict([target_sequence] + [encoder_states])\n",
        "\n",
        "        # Palavra com maior probabilidade\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer_en.index_word[sampled_token_index]\n",
        "\n",
        "        # Adiciona a palavra à sentença decodificada\n",
        "        if sampled_word == \"<end>\":\n",
        "            break\n",
        "        decoded_sentence.append(sampled_word)\n",
        "\n",
        "        # Atualize a sequência de entrada para o próximo passo\n",
        "        target_sequence = np.zeros((1, 1))\n",
        "        target_sequence[0, 0] = sampled_token_index\n",
        "\n",
        "        # Atualize os estados do decoder\n",
        "        encoder_states = h\n",
        "\n",
        "    return \" \".join(decoded_sentence)\n"
      ],
      "metadata": {
        "id": "b_kiueRbrj6P"
      },
      "id": "b_kiueRbrj6P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste de uma frase simples"
      ],
      "metadata": {
        "id": "-8gfbaw6rw6e"
      },
      "id": "-8gfbaw6rw6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Avaliação"
      ],
      "metadata": {
        "id": "amYYb7Q2oZMa"
      },
      "id": "amYYb7Q2oZMa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliaremos o modelo usando o BLEU score, novamente. Espera-se que a tradução seja mais eficiente pelo fato de RNNs serem capazes de capturar o contexto das frases, gerando traduções mais naturais e eficientes."
      ],
      "metadata": {
        "id": "amBvIRASob7m"
      },
      "id": "amBvIRASob7m"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}