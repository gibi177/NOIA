{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ed281b69-6941-4746-8fcb-edb34d3f03b5",
      "metadata": {
        "id": "ed281b69-6941-4746-8fcb-edb34d3f03b5"
      },
      "source": [
        "# Tradução por Máquina: método estatístico e codificador-decodificador"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c599ca7c-fbff-4dd6-a4a3-553a0feb60ac",
      "metadata": {
        "id": "c599ca7c-fbff-4dd6-a4a3-553a0feb60ac"
      },
      "source": [
        "### Trabalho 3 da disciplina Noções de Inteligência Artificial - 2/2024\n",
        "### Alunos: Felipe Lopes Gibin Duarte (231025207) e Matheus das Neves Fernandes (231013672)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e92ee94-5f59-466b-aa2b-818f1e810f9e",
      "metadata": {
        "id": "9e92ee94-5f59-466b-aa2b-818f1e810f9e"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a562ae-df7e-47c3-851d-a72321bc1fe1",
      "metadata": {
        "id": "e3a562ae-df7e-47c3-851d-a72321bc1fe1"
      },
      "source": [
        "Neste trabalho vamos usar duas técnicas de tradução automática, a primeira puramente\n",
        "estatística e a segunda usando ML na arquitetura codificador-decodificador. Vamos analisar a eficácia de ambas, destacando aspectos teóricos importantes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7409cbdf-0f8b-4718-8e7f-cb26a9df494b",
      "metadata": {
        "id": "7409cbdf-0f8b-4718-8e7f-cb26a9df494b"
      },
      "source": [
        "## 1. Base de Dados"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b53ee5a2-9de8-4599-9a0d-248e6d192073",
      "metadata": {
        "id": "b53ee5a2-9de8-4599-9a0d-248e6d192073"
      },
      "source": [
        "### 1.1 Download da base de dados\n",
        "\n",
        "Usaremos a base Português - Inglês do parlamento europeu, disponível em https://www.statmt.org/europarl/. Encontre o link “parallel corpus Portuguese-English” e extraia o condeúdo do zip. Pela falta de recursos computacionais, o tamanho dos arquivos foi reduzido para 11 mil sentenças cada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7887fa99-d614-4840-b9c7-5c59b543eb2b",
      "metadata": {
        "id": "7887fa99-d614-4840-b9c7-5c59b543eb2b"
      },
      "source": [
        "### 1.2 Upload dos arquivos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0e788379-61d8-4e48-b74d-0b8ccb15e670",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "id": "0e788379-61d8-4e48-b74d-0b8ccb15e670",
        "outputId": "133c0c4c-047f-48a2-9c14-a49b57100298"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ff522658-6da4-4d16-994b-c2a776989ac7\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ff522658-6da4-4d16-994b-c2a776989ac7\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving reduced_europarl_pt.txt to reduced_europarl_pt (1).txt\n",
            "Saving reduced_europarl_en.txt to reduced_europarl_en (1).txt\n"
          ]
        }
      ],
      "source": [
        "#Upload dos arquivos extraídos para o google colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.3 Processamento dos arquivos"
      ],
      "metadata": {
        "id": "JQhuNIwr7YEZ"
      },
      "id": "JQhuNIwr7YEZ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Le os arquivos e remove line breaks\n",
        "with open('/content/reduced_europarl_pt.txt', 'r', encoding='utf-8') as pt_file:\n",
        "    portuguese_sentences = [line.strip() for line in pt_file.readlines()]\n",
        "\n",
        "with open('/content/reduced_europarl_en.txt', 'r', encoding='utf-8') as en_file:\n",
        "    english_sentences = [line.strip() for line in en_file.readlines()]"
      ],
      "metadata": {
        "id": "nh4VrD6y2MlX"
      },
      "id": "nh4VrD6y2MlX",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove pontuação e converte para letras minusculas\n",
        "import re\n",
        "def clean_text(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub(r'[^\\w\\s]', '', text)\n",
        "  return text\n",
        "\n",
        "# Exibir algumas linhas para verificar\n",
        "print(\"Frases em português:\", portuguese_sentences[:5])\n",
        "print(\"Frases em inglês:\", english_sentences[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbvJjm-E3FWj",
        "outputId": "79f68f44-b70e-49a1-c6c8-af2e41619d11"
      },
      "id": "TbvJjm-E3FWj",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frases em português: ['Reinício da sessão', 'Declaro reaberta a sessão do Parlamento Europeu, que tinha sido interrompida na sexta-feira, 17 de Dezembro último, e renovo todos os meus votos, esperando que tenham tido boas férias.', 'Como puderam constatar, o grande \"bug do ano 2000\" não aconteceu. Em contrapartida, os cidadãos de alguns dos nossos países foram vítimas de catástrofes naturais verdadeiramente terríveis.', 'Os senhores manifestaram o desejo de se proceder a um debate sobre o assunto nos próximos dias, durante este período de sessões.', 'Entretanto, gostaria - como também me foi pedido por um certo número de colegas - que observássemos um minuto de silêncio por todas as vítimas, nomeadamente das tempestades, nos diferentes países da União Europeia que foram afectados.']\n",
            "Frases em inglês: ['Resumption of the session', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.', \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f7e8c41-2c2c-4564-b4ea-87e534a30b4e",
      "metadata": {
        "id": "3f7e8c41-2c2c-4564-b4ea-87e534a30b4e"
      },
      "source": [
        "## 2. IBM Model 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f58d0ff-9cbc-40d8-87a4-14304f2cc5c5",
      "metadata": {
        "id": "5f58d0ff-9cbc-40d8-87a4-14304f2cc5c5"
      },
      "source": [
        "O IBM Model 1 é um modelo complexo de tradução estatística, amplamente utilizado antes da popularização das traduções baseadas em redes neurais. Ele se baseia em probabilidades de palavras individuais e sua correspondência entre os idiomas, ou seja, o modelo estima a melhor tradução palavra por palavra."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 Treinando o modelo IBM Model 1"
      ],
      "metadata": {
        "id": "QjcsqBCGASqp"
      },
      "id": "QjcsqBCGASqp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d459c5dd-aa36-4e12-ab7c-f34e7b6fc6d3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d459c5dd-aa36-4e12-ab7c-f34e7b6fc6d3",
        "outputId": "a04a80a0-9f78-465a-da53-dc3577cbeacf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo treinado com sucesso!\n"
          ]
        }
      ],
      "source": [
        "#Instalando o nltk\n",
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "from nltk.translate import IBMModel1\n",
        "from nltk.translate import AlignedSent\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab') #Necessario para a tokenização\n",
        "\n",
        "portuguese_tokens = [word_tokenize(sentence) for sentence in portuguese_sentences]\n",
        "english_tokens = [word_tokenize(sentence) for sentence in english_sentences]\n",
        "\n",
        "# Criar o corpus alinhado em pares de listas\n",
        "parallel_corpus = [AlignedSent(pt, en) for pt, en in zip(portuguese_tokens, english_tokens)]\n",
        "\n",
        "# Treinar o modelo IBM Model 1\n",
        "model = IBMModel1(parallel_corpus, 5)  # 5 iterações de treinamento\n",
        "\n",
        "print(\"Modelo treinado com sucesso!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 Avaliando a qualidade da tradução"
      ],
      "metadata": {
        "id": "UlNbYdnfDOjO"
      },
      "id": "UlNbYdnfDOjO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e2f9554-530b-4f31-9805-8b7b5fb0c046",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e2f9554-530b-4f31-9805-8b7b5fb0c046",
        "outputId": "3b24abaa-b206-412c-c43a-afbdf34bb3f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Média do BLEU score para todas as sentenças: 0.0206\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate import IBMModel1, AlignedSent\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Suavização para BLEU\n",
        "smoothing = SmoothingFunction()\n",
        "\n",
        "# Função para traduzir uma sentença usando o IBM Model 1\n",
        "def translate_sentence(model, sentence):\n",
        "    translated_tokens = []\n",
        "    for word in sentence:\n",
        "        # Verificar se a palavra existe na tabela de tradução\n",
        "        if word in model.translation_table and model.translation_table[word]:\n",
        "            # Obter a melhor tradução\n",
        "            best_translation = max(model.translation_table[word], key=model.translation_table[word].get)\n",
        "        else:\n",
        "            best_translation = word  # Fallback: palavra original\n",
        "        translated_tokens.append(best_translation)\n",
        "    return translated_tokens\n",
        "\n",
        "# Inicializar referências e hipóteses\n",
        "references = []\n",
        "hypotheses = []\n",
        "\n",
        "# Iterar sobre as sentenças da base\n",
        "for pt_sentence, en_sentence in zip(portuguese_sentences, english_sentences):\n",
        "    # Tokenizar as sentenças\n",
        "    pt_tokens = pt_sentence.split()\n",
        "    en_tokens = en_sentence.split()\n",
        "\n",
        "    # Traduzir a sentença em português\n",
        "    translated_tokens = translate_sentence(model, pt_tokens)\n",
        "\n",
        "    # Adicionar ao conjunto de referências e hipóteses\n",
        "    references.append([en_tokens])  # Referência: lista de listas (uma lista por referência)\n",
        "    hypotheses.append(translated_tokens)  # Hipótese: lista de palavras traduzidas\n",
        "\n",
        "# Calcular o BLEU score para cada sentença\n",
        "bleu_scores = [\n",
        "    sentence_bleu(ref, hyp, smoothing_function=smoothing.method1)\n",
        "    for ref, hyp in zip(references, hypotheses)\n",
        "]\n",
        "\n",
        "# Calcular a média do BLEU score\n",
        "average_bleu_score = sum(bleu_scores) / len(bleu_scores)\n",
        "print(f\"Média do BLEU score para todas as sentenças: {average_bleu_score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 Considerações sobre o BLEU score"
      ],
      "metadata": {
        "id": "FYubUAnfQK9y"
      },
      "id": "FYubUAnfQK9y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "O BLEU score (Bilingual Evaluation Understudy) é uma métrica usada para avaliar a qualidade de traduções automáticas, comparando a tradução gerada pelo modelo com uma ou mais traduções de referência. No caso do IBM model 1, o resultado encontrado foi desapontador devido à abordagem de tradução palavra a palavra, que é ineficiente. Alem disso, a limitação em tamanho da base de dados devido à restrições de capacidade computacional foi outro fator relevante para o score médio ter sido tão baixo (0.0206).  "
      ],
      "metadata": {
        "id": "WyOLAzhBQZD6"
      },
      "id": "WyOLAzhBQZD6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Codificador-Decodificador"
      ],
      "metadata": {
        "id": "uZ1NvUh2RqH0"
      },
      "id": "uZ1NvUh2RqH0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Upload e processamento dos dados"
      ],
      "metadata": {
        "id": "mANyV_sRCUoP"
      },
      "id": "mANyV_sRCUoP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir algumas linhas para verificar\n",
        "print(\"Frases em português:\", portuguese_sentences[:5])\n",
        "print(\"Frases em inglês:\", english_sentences[:5])"
      ],
      "metadata": {
        "id": "w43QCYY1DEKT",
        "outputId": "53198997-b055-4410-fc1a-6f14491604f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "w43QCYY1DEKT",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frases em português: ['Reinício da sessão', 'Declaro reaberta a sessão do Parlamento Europeu, que tinha sido interrompida na sexta-feira, 17 de Dezembro último, e renovo todos os meus votos, esperando que tenham tido boas férias.', 'Como puderam constatar, o grande \"bug do ano 2000\" não aconteceu. Em contrapartida, os cidadãos de alguns dos nossos países foram vítimas de catástrofes naturais verdadeiramente terríveis.', 'Os senhores manifestaram o desejo de se proceder a um debate sobre o assunto nos próximos dias, durante este período de sessões.', 'Entretanto, gostaria - como também me foi pedido por um certo número de colegas - que observássemos um minuto de silêncio por todas as vítimas, nomeadamente das tempestades, nos diferentes países da União Europeia que foram afectados.']\n",
            "Frases em inglês: ['Resumption of the session', 'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.', \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\", 'You have requested a debate on this subject in the course of the next few days, during this part-session.', \"In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Importações e configurações\n"
      ],
      "metadata": {
        "id": "FuKiFoj4D_tV"
      },
      "id": "FuKiFoj4D_tV"
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense, Input\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Configurações\n",
        "max_vocab_size = 25000  # Tamanho máximo do vocabulário\n",
        "max_seq_length = 30     # Tamanho máximo da sequência\n",
        "embedding_dim = 256     # Dimensão do embedding\n",
        "latent_dim = 512        # Dimensão do vetor latente (de contexto) do RNN\n",
        "batch_size = 64\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "lneHetUNECjg"
      },
      "id": "lneHetUNECjg",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Tokenização e padding"
      ],
      "metadata": {
        "id": "MHuDJz-kETMA"
      },
      "id": "MHuDJz-kETMA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenização converte palavras em números, onde cada número representa o índice de uma palavra no vocabulário. Palavras associadas a índices de número mais baixo são mais frequentes no texto. O token \"start\" e \"end\" indicam onde a tradução começa e onde termina. O padding garante que todas as sentenças tenham o mesmo comprimento, se for uma frase mais curta que max__seq_length, serão adicionados zeros ao final das frases."
      ],
      "metadata": {
        "id": "CROF6ckNXiqk"
      },
      "id": "CROF6ckNXiqk"
    },
    {
      "cell_type": "code",
      "source": [
        "#Adicionamos tokens de inicio e fim em ingles\n",
        "english_sentences = [\"<start> \" + sentence.strip() + \" <end>\" for sentence in english_sentences]\n",
        "print(english_sentences[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZxoWZu3YitB",
        "outputId": "cd3ff247-e760-423a-e73c-e7bacc4d4c18"
      },
      "id": "0ZxoWZu3YitB",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start> I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period. <end>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenização\n",
        "tokenizer_pt = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer_pt.fit_on_texts(portuguese_sentences)\n",
        "\n",
        "tokenizer_en = Tokenizer(num_words=max_vocab_size)\n",
        "tokenizer_en.fit_on_texts(english_sentences)"
      ],
      "metadata": {
        "id": "329Rfw94EW4C"
      },
      "id": "329Rfw94EW4C",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sequências numericas das sentenças em portugues e ingles\n",
        "input_sequences = tokenizer_pt.texts_to_sequences(portuguese_sentences)\n",
        "target_sequences = tokenizer_en.texts_to_sequences(english_sentences)"
      ],
      "metadata": {
        "id": "VAwr3oP4FDrM"
      },
      "id": "VAwr3oP4FDrM",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adiciona padding ao final das sentenças, de forma a padronizar o tamanho delas\n",
        "input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding=\"post\")\n",
        "target_sequences = pad_sequences(target_sequences, maxlen=max_seq_length, padding=\"post\")\n"
      ],
      "metadata": {
        "id": "Rd-fo7OjFTe0"
      },
      "id": "Rd-fo7OjFTe0",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criação dos arrays de entrada e saida\n",
        "encoder_input_data = np.array(input_sequences)\n",
        "decoder_input_data = np.array(target_sequences[:, :-1])  # Sem o token <end>\n",
        "decoder_target_data = np.array(target_sequences[:, 1:])  # Sem o token <start>, usado pra prever a próxima palavra"
      ],
      "metadata": {
        "id": "FlE5--vZeeQg"
      },
      "id": "FlE5--vZeeQg",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Estrutura do modelo encoder-decoder"
      ],
      "metadata": {
        "id": "DwMCX73mcd7Z"
      },
      "id": "DwMCX73mcd7Z"
    },
    {
      "cell_type": "markdown",
      "source": [
        "O encoder processa a sequência de entrada (sentenças em português) e gera um vetor de contexto que contém a representação da sequência. O vetor de contexto é então passado para o decodificador. Neste caso, encoder_outputs pode ser ignorado, só estamos interessados no vetor state_h."
      ],
      "metadata": {
        "id": "DgdHzOINc0aE"
      },
      "id": "DgdHzOINc0aE"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o encoder\n",
        "\n",
        "# A entrada é uma sequência de índices (tokens) representando palavras em português\n",
        "encoder_inputs = Input(shape=(max_seq_length,))\n",
        "\n",
        "#Embedding para representar cada palavra como um vetor\n",
        "encoder_embedding = Embedding(input_dim=max_vocab_size, output_dim=embedding_dim)(encoder_inputs)\n",
        "\n",
        "#RNN que vai gerar o vetor de estado\n",
        "encoder_rnn = SimpleRNN(latent_dim, return_state=True)\n",
        "encoder_outputs, state_h = encoder_rnn(encoder_embedding)"
      ],
      "metadata": {
        "id": "7-KpaTeVFljs"
      },
      "id": "7-KpaTeVFljs",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O decoder usa o estado latente (state_h) gerado pelo encoder para produzir a tradução. Ele precisa ser alimentado com o token \"start\" no início, gerando uma palavra por vez até encontrar o token \"end\"."
      ],
      "metadata": {
        "id": "n4Wty8jNgNE2"
      },
      "id": "n4Wty8jNgNE2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definindo o decoder\n",
        "\n",
        "#Sequencia de tokens em ingles, sem o <end>\n",
        "decoder_inputs = Input(shape=(max_seq_length,))\n",
        "\n",
        "#Embedding dos tokens em ingles\n",
        "decoder_embedding = Embedding(input_dim=max_vocab_size, output_dim=embedding_dim)(decoder_inputs)\n",
        "\n",
        "#RNN do decoder\n",
        "#decoder_outputs: Saída para cada passo temporal da RNN no decoder.\n",
        "#A RNN usa o estado final do encoder (state_h) como estado inicial.\n",
        "decoder_rnn = SimpleRNN(latent_dim, return_sequences=True)\n",
        "decoder_outputs = decoder_rnn(decoder_embedding, initial_state=state_h)\n",
        "\n",
        "#Camada densa para prever a próxima palavra\n",
        "# A softmax prevê a próxima palavra em inglês como uma probabilidade sobre o vocabulário\n",
        "decoder_dense = Dense(max_vocab_size, activation=\"softmax\")\n",
        "decoder_outputs = decoder_dense(decoder_outputs)"
      ],
      "metadata": {
        "id": "ErJmpm55ihec"
      },
      "id": "ErJmpm55ihec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora, combinamos o encoder e o decoder para montar o modelo completo que mapeará as frases em ingles para as suas traduções em inglês."
      ],
      "metadata": {
        "id": "YsNwpU_1lhR5"
      },
      "id": "YsNwpU_1lhR5"
    },
    {
      "cell_type": "code",
      "source": [
        "#O modelo recebe como entrada encoder_inputs e decoder_inputs e tem como saida decoder_outputs (frases em ingles traduzidas)\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
      ],
      "metadata": {
        "id": "X__WpG14l8BW"
      },
      "id": "X__WpG14l8BW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Treinamento"
      ],
      "metadata": {
        "id": "H4YKgGBDm7-J"
      },
      "id": "H4YKgGBDm7-J"
    },
    {
      "cell_type": "code",
      "source": [
        "#Compilação do modelo completo\n",
        "\n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "8yXhbiQnn_f-"
      },
      "id": "8yXhbiQnn_f-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Treinamento do modelo\n",
        "\n",
        "model.fit(\n",
        "    [encoder_input_data, decoder_input_data],\n",
        "    decoder_target_data,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_split=0.2  # 20% para validação.\n",
        ")"
      ],
      "metadata": {
        "id": "GUB47Y9VoAIP"
      },
      "id": "GUB47Y9VoAIP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Tradução de frases simples"
      ],
      "metadata": {
        "id": "Ym_SeARYrPHc"
      },
      "id": "Ym_SeARYrPHc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A arquitetura que criamos é para o treinamento. Para usar o modelo treinado na prática, precisamos configurar o encoder e decoder para inferência. Na inferência (tradução), o processo é diferente porque não temos a frase-alvo completa (em inglês). A tradução precisa ser gerada uma palavra por vez, de forma iterativa."
      ],
      "metadata": {
        "id": "7WOY_QyxtQqE"
      },
      "id": "7WOY_QyxtQqE"
    },
    {
      "cell_type": "code",
      "source": [
        "#Encoder para inferencia\n",
        "encoder_model = Model(encoder_inputs, encoder_states)"
      ],
      "metadata": {
        "id": "aPwztc8ouSCi"
      },
      "id": "aPwztc8ouSCi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder para inferencia\n",
        "#Recebe os estados latentes do encoder como entrada e gera palavras passo a passo\n",
        "\n",
        "# Entradas para os estados iniciais do decoder\n",
        "decoder_state_input = Input(shape=(latent_dim,))\n",
        "\n",
        "# Entrada da palavra atual\n",
        "decoder_single_input = Input(shape=(1,))  # Entrada com uma palavra por vez\n",
        "\n",
        "# Embedding para a palavra atual\n",
        "decoder_embedded = decoder_embedding(decoder_single_input)\n",
        "\n",
        "# RNN usando o estado anterior\n",
        "decoder_output, decoder_state = decoder_rnn(decoder_embedded, initial_state=decoder_state_input)\n",
        "\n",
        "# Camada densa para prever a próxima palavra\n",
        "decoder_prediction = decoder_dense(decoder_output)\n",
        "\n",
        "# Modelo do decoder para inferência\n",
        "decoder_model = Model(\n",
        "    [decoder_single_input, decoder_state_input],  # Entradas: palavra + estado anterior\n",
        "    [decoder_prediction, decoder_state]          # Saídas: previsão + estado atual\n",
        ")"
      ],
      "metadata": {
        "id": "xFrfez88ubyG"
      },
      "id": "xFrfez88ubyG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criaremos uma função de tradução que passa uma frase em portugûes e retorna a frase traduzida para o inglês. Após isso, traduziremos uma frase simples"
      ],
      "metadata": {
        "id": "tasAEIpGrUmB"
      },
      "id": "tasAEIpGrUmB"
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(input_sequence, encoder_model, decoder_model, tokenizer_en, max_seq_length):\n",
        "    # Obtenha o estado latente do encoder\n",
        "    encoder_states = encoder_model.predict(input_sequence)\n",
        "\n",
        "    # Comece a tradução com o token <start>\n",
        "    target_sequence = np.zeros((1, 1))  # Tamanho (1 frase, 1 palavra)\n",
        "    target_sequence[0, 0] = tokenizer_en.word_index[\"<start>\"]\n",
        "\n",
        "    decoded_sentence = []\n",
        "\n",
        "    # Iterar para gerar palavras até o token <end> ou o limite de comprimento\n",
        "    for _ in range(max_seq_length):\n",
        "        # Previsão da próxima palavra\n",
        "        output_tokens, h = decoder_model.predict([target_sequence] + [encoder_states])\n",
        "\n",
        "        # Palavra com maior probabilidade\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_word = tokenizer_en.index_word[sampled_token_index]\n",
        "\n",
        "        # Adiciona a palavra à sentença decodificada\n",
        "        if sampled_word == \"<end>\":\n",
        "            break\n",
        "        decoded_sentence.append(sampled_word)\n",
        "\n",
        "        # Atualize a sequência de entrada para o próximo passo\n",
        "        target_sequence = np.zeros((1, 1))\n",
        "        target_sequence[0, 0] = sampled_token_index\n",
        "\n",
        "        # Atualize os estados do decoder\n",
        "        encoder_states = h\n",
        "\n",
        "    return \" \".join(decoded_sentence)\n"
      ],
      "metadata": {
        "id": "b_kiueRbrj6P"
      },
      "id": "b_kiueRbrj6P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Teste de uma frase simples"
      ],
      "metadata": {
        "id": "-8gfbaw6rw6e"
      },
      "id": "-8gfbaw6rw6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Avaliação"
      ],
      "metadata": {
        "id": "amYYb7Q2oZMa"
      },
      "id": "amYYb7Q2oZMa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Avaliaremos o modelo usando o BLEU score, novamente. Espera-se que a tradução seja mais eficiente pelo fato de RNNs serem capazes de capturar o contexto das frases, gerando traduções mais naturais e eficientes."
      ],
      "metadata": {
        "id": "amBvIRASob7m"
      },
      "id": "amBvIRASob7m"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}